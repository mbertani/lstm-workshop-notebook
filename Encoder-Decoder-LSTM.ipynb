{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "In the following, I have collected information from several sources. These are spread around in this notebook. The resulting text written here is thus a collection of what these authors have written in and between the lines. The misprints and imprecisions are mine though. Please do follow the links if what is written here doesn't make sense.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq model - Learn to Execute\n",
    "\n",
    "**Source:** This is a notebook version of https://machinelearningmastery.com/learn-add-numbers-seq2seq-recurrent-neural-networks/ with some minor modifications. I'm not including the part of \"the beginner's mistake\" from this link, so you should take a look at that too.\n",
    "\n",
    "If you want to go deeper, you can look at https://machinelearningmastery.com/lstms-with-python/ , chapter 9.\n",
    "\n",
    "The [Encoder-Decoder LSTM](https://machinelearningmastery.com/encoder-decoder-long-short-term-memory-networks/) was developed for natural language processing problems where it demonstrated state-of-the-art performance, specifically in the area of text translation called statistical machine translation.\n",
    "\n",
    "The list below highlights some interesting applications of the Encoder-Decoder LSTM architecture:\n",
    "\n",
    "- Machine Translation, e.g. English to French translation of phrases.\n",
    "- Learning to Execute, e.g. calculate the outcome of small programs.\n",
    "- Image Captioning, e.g. generating a text description for images.\n",
    "- Conversational Modeling, e.g. generating answers to textual questions.\n",
    "- Movement Classification, e.g. generating a sequence of commands from a sequence of gestures.\n",
    "\n",
    "\n",
    "## Architecture of encoder-decoder LSTM\n",
    "\n",
    "The model is comprised of two parts: the _encoder_ and the _decoder_. First, the input sequence is\n",
    "shown to the network one encoded character at a time. We need an encoding level to learn the\n",
    "relationship between the steps in the input sequence and develop an internal representation of\n",
    "these relationships.\n",
    "One or more LSTM layers can be used to implement the encoder model. In the example we will use only one. The output of this\n",
    "model is a fixed-size vector that represents the internal representation of the input sequence.\n",
    "The number of memory cells in this layer defines the length of this fixed-sized vector.\n",
    "\n",
    "**Side note:** This fixed-size vector seems to cause performance problems when working on long input or output sequences. To fix this, one can use [Attention](https://machinelearningmastery.com/implementation-patterns-encoder-decoder-rnn-architecture-attention/) which is not covered in this notebook. But here is an [example](https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/) to get you started.\n",
    "\n",
    "The decoder must transform the learned internal representation of the input sequence into\n",
    "the correct output sequence. One or more LSTM layers can also be used to implement the\n",
    "decoder model. In the example, we will use only one. This model reads from the fixed sized output from the encoder model. Then we use a Dense layer as the output for the network. The same weights can\n",
    "be used to output each time step in the output sequence by wrapping the Dense layer in a\n",
    "[TimeDistributed wrapper](https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/). You can also learn more about this in the keras documentation for [TimeDistributed](https://keras.io/layers/wrappers/#timedistributed).\n",
    "\n",
    "The output from the encoder is a 2D vector. The input of the decoder is a 3D vector. To make them fit, we use an adaptor: the [RepeatVector](https://keras.io/layers/core/#repeatvector).\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(LSTM(..., input_shape=(...)))  # encoder, output 2D\n",
    "model.add(RepeatVector(...))  # adaptor, map 2D to 3D by repeating the input. \n",
    "model.add(LSTM(..., return_sequences=True))  # decoder, input 3D\n",
    "model.add(TimeDistributed(Dense(...)))  # we reuse the same output layer for each element in the output sequence. \n",
    "                                     \n",
    "```\n",
    "\n",
    "## Addition Prediction Problem\n",
    "We are going to work on an example of _Learning to Execute_, by calculating the sum output of two input numbers. This problem was used by Wojciech Zaremba and Ilya Sutskever in their 2014 paper, where this architecture was demostrated learning to calculate the output of small programs.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "Input: ['1', '3', '+', '6']\n",
    "Output: ['1', '9']\n",
    "```\n",
    "\n",
    "For the example we are going to proceed with the following steps:\n",
    "\n",
    "_Data processing_: 1-6\n",
    "\n",
    "_Model processing_: 7-11\n",
    "\n",
    "1. Generate Sum Pairs.\n",
    "2. Integers to Padded Strings.\n",
    "3. Integer Encoded Sequences.\n",
    "4. One Hot Encoded Sequences.\n",
    "5. Sequence Generation Pipeline.\n",
    "6. Decode Sequences.\n",
    "7. Problem definition.\n",
    "8. Model compilation.\n",
    "9. Fit the model.\n",
    "10. Evaluate the model.\n",
    "11. Make predictions.\n",
    "\n",
    "## Further reading\n",
    "- [Learning to Execute, 2015](http://arxiv.org/abs/1410.4615).\n",
    "- [Sequence to Sequence Learning with Neural Networks, 2014](https://arxiv.org/abs/1409.3215).\n",
    "- [Show and Tell: A Neural Image Caption Generator, 2014](https://arxiv.org/abs/1411.4555).\n",
    "- [Caption this, with TensorFlow!](https://github.com/mlberkeley/oreilly-captions)\n",
    "- [Keras example for addition with RNNs](https://github.com/keras-team/keras/blob/master/examples/addition_rnn.py)\n",
    "\n",
    "\n",
    "## Homework\n",
    "- Increase the number of terms or number of digits and tune the model to get 100% accuracy.\n",
    "- Update the example to support a variable number of terms in a given instance and tune a model to get 100% accuracy.\n",
    "- Add support for other math operations like subtraction, division, and multiplication.\n",
    "- If you feel brave enough, take the \"Caption this, with Tensorflow!\" link. Can you make it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generate Sum Pairs\n",
    "We need a function to generate a sequence of random integers and their sum.\n",
    "\n",
    "Look at the following code, do you understand what is going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randint\n",
    "\n",
    "# generate lists of random integers and their sum\n",
    "def random_sum_pairs(n_examples, n_numbers, largest):\n",
    "    X, y = list(), list()\n",
    "    for i in range(n_examples):\n",
    "        in_pattern = [randint(1,largest) for _ in range(n_numbers)]\n",
    "        out_pattern = sum(in_pattern)\n",
    "        X.append(in_pattern)\n",
    "        y.append(out_pattern)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the above function, play with the parameters a bit.\n",
    "seed(1)\n",
    "n_samples = 1\n",
    "n_numbers = 2\n",
    "largest = 10\n",
    "# generate pairs\n",
    "X, y = random_sum_pairs(n_samples, n_numbers, largest)\n",
    "print(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Integers to Padded Strings\n",
    "We need to convert the integers to strings. \n",
    "The important part here is that we need strings of the same size for the input and ouput. That is, we need to pad the data with a character, different from the data, so that the model learns to ingore it. We will use the space character ' ' for padding the string on the left, keeping the information far on the right.\n",
    "\n",
    "**Homework**: Try different paddings and check the performance of the final model!\n",
    "\n",
    "Padding requires we know the length of the biggest sequence. Since we write in base 10, we can take the ```log10()``` of the largest integer we can generate, times the number of summands ```n_numbers```, and add the right amount of plus symbols (```n_numbers - 1```). Taking care of the technicalities, we wet the following formula:\n",
    "\n",
    "```python\n",
    "max_length = n_numbers * ceil(log10(largest+1)) + n_numbers - 1\n",
    "```\n",
    "For example, if we assume we have numbers up to 10, with 3 summands, the longest sequence we can write is thus ```['1','0','+','1','0','+','1','0']``` for which the formula give us \n",
    "```python\n",
    "max_length = 3 * ceil(log10(10+1)) + 3 - 1 = 3*2 + 3 - 1 = 8\n",
    "```\n",
    "\n",
    "Check the following code for converting lists of numbers to padded strings. Do you follow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from math import log10\n",
    "\n",
    "# convert data to strings\n",
    "def to_string(X, y, n_numbers, largest):\n",
    "\tmax_length = n_numbers * ceil(log10(largest+1)) + n_numbers - 1\n",
    "\tXstr = list()\n",
    "\tfor pattern in X:\n",
    "\t\tstrp = '+'.join([str(n) for n in pattern])\n",
    "\t\tstrp = ''.join([' ' for _ in range(max_length-len(strp))]) + strp\n",
    "\t\tXstr.append(strp)\n",
    "\tmax_length = ceil(log10(n_numbers * (largest+1)))\n",
    "\tystr = list()\n",
    "\tfor pattern in y:\n",
    "\t\tstrp = str(pattern)\n",
    "\t\tstrp = ''.join([' ' for _ in range(max_length-len(strp))]) + strp\n",
    "\t\tystr.append(strp)\n",
    "\treturn Xstr, ystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following code to test our functions so far. Feel free to play with the parameters.\n",
    "\n",
    "seed(1)\n",
    "n_samples = 1\n",
    "n_numbers = 2\n",
    "largest = 10\n",
    "# generate pairs\n",
    "X, y = random_sum_pairs(n_samples, n_numbers, largest)\n",
    "print(X, y)\n",
    "# convert to strings\n",
    "X, y = to_string(X, y, n_numbers, largest)\n",
    "print(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Integer Encoded Sequences\n",
    "\n",
    "Neural networks are mathematical functions that take numbers as inputs. So we need to convert our strings to integers.\n",
    "Integer encoding transforms the problem into a classification problem where the output sequence may be considered class outputs with 11 possible values each.\n",
    "\n",
    "We can use the following alphabet:\n",
    "```python\n",
    "alphabet = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', ' ']\n",
    "```\n",
    "\n",
    "Look at the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# integer encode strings\n",
    "def integer_encode(X, y, alphabet):\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    Xenc = list()\n",
    "    for pattern in X:\n",
    "        integer_encoded = [char_to_int[char] for char in pattern]\n",
    "        Xenc.append(integer_encoded)\n",
    "    yenc = list()\n",
    "    for pattern in y:\n",
    "        integer_encoded = [char_to_int[char] for char in pattern]\n",
    "        yenc.append(integer_encoded)\n",
    "    return Xenc, yenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We test our code so far\n",
    "\n",
    "seed(1)\n",
    "n_samples = 1\n",
    "n_numbers = 2\n",
    "largest = 10\n",
    "# generate pairs\n",
    "X, y = random_sum_pairs(n_samples, n_numbers, largest)\n",
    "print(X, y)\n",
    "# convert to strings\n",
    "X, y = to_string(X, y, n_numbers, largest)\n",
    "print(X, y)\n",
    "\n",
    "# integer encode\n",
    "alphabet = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', ' ']\n",
    "X, y = integer_encode(X, y, alphabet)\n",
    "print(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. One Hot Encoded Sequences\n",
    "\n",
    "[Wait what?](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f) I did swallow the number argument in the previos step, but what is this one-hot-thingy and why do i need it? Take a look at the link and come back to the excercise. \n",
    "\n",
    "So, in short, we do not want the natural order of the categories to mess up our prediction problem. So a binary representation of the input will make sure there is no correlation between the categories, in our case, our digits.\n",
    "\n",
    "This involves converting each integer to a binary vector with the same length as the alphabet and marking the speci\f",
    "c integer with a 1. For example, a 0 integer represents the '0' character and would be encoded as a\n",
    "binary vector with a 1 in the 0th position of an 11 element vector: [1, 0, 0, 0, 0, 0, 0, 0,0, 0, 0, 0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode\n",
    "def one_hot_encode(X, y, max_int):\n",
    "    Xenc = list()\n",
    "    for seq in X:\n",
    "        pattern = list()\n",
    "        for index in seq:\n",
    "            vector = [0 for _ in range(max_int)]\n",
    "            vector[index] = 1\n",
    "            pattern.append(vector)\n",
    "        Xenc.append(pattern)\n",
    "    yenc = list()\n",
    "    for seq in y:\n",
    "        pattern = list()\n",
    "        for index in seq:\n",
    "            vector = [0 for _ in range(max_int)]\n",
    "            vector[index] = 1\n",
    "            pattern.append(vector)\n",
    "        yenc.append(pattern)\n",
    "    return Xenc, yenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following tests our code so far.\n",
    "\n",
    "seed(1)\n",
    "n_samples = 1\n",
    "n_numbers = 2\n",
    "largest = 10\n",
    "# generate pairs\n",
    "X, y = random_sum_pairs(n_samples, n_numbers, largest)\n",
    "print(X, y)\n",
    "# convert to strings\n",
    "X, y = to_string(X, y, n_numbers, largest)\n",
    "print(X, y)\n",
    "# integer encode\n",
    "alphabet = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', ' ']\n",
    "X, y = integer_encode(X, y, alphabet)\n",
    "print(X, y)\n",
    "# one hot encode\n",
    "X, y = one_hot_encode(X, y, len(alphabet))\n",
    "print(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sequence Generation Pipeline\n",
    "\n",
    "We wrap everything we have so far in a singe function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "\n",
    "# generate an encoded dataset\n",
    "def generate_data(n_samples, n_numbers, largest, alphabet):\n",
    "    # generate pairs\n",
    "    X, y = random_sum_pairs(n_samples, n_numbers, largest)\n",
    "    # convert to strings\n",
    "    X, y = to_string(X, y, n_numbers, largest)\n",
    "    # integer encode\n",
    "    X, y = integer_encode(X, y, alphabet)\n",
    "    # one hot encode\n",
    "    X, y = one_hot_encode(X, y, len(alphabet))\n",
    "    # return as numpy arrays\n",
    "    X, y = array(X), array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Decode Sequences\n",
    "Finally, we need to make sense of what we get out of our neural network, and decode back the output vectors into numbers. We first reverse the one-hot-encoding via the ```argmax``` function, and then use the reverse mapping from our alphabet. The following function takes care of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "\n",
    "# invert encoding\n",
    "def invert(seq, alphabet):\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "    strings = list()\n",
    "    for pattern in seq:\n",
    "        string = int_to_char[argmax(pattern)]\n",
    "        strings.append(string)\n",
    "    return ''.join(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Problem definition\n",
    "We need to define the specifications of the sequence prediction problem. We will use the following variables:\n",
    "- **n_numbers**: The number of terms in the equation, (e.g. 2 for 10+10).\n",
    "- **largest**: The largest numerical value for each term (e.g. 10 for values between 1-10).\n",
    "- **alphabet**: The symbols used to encode the input and output sequences (e.g. 0-9, + and ' ')\n",
    "\n",
    "We will use a configuration of the problem that has a modest complexity. Each instance will\n",
    "be comprised of 3 terms with the maximum value of 10 per term. The alphabet remains fixed\n",
    "regardless of configuration with the values 0-9, '+', and ' '.\n",
    "\n",
    "The network needs three configuration values defined by the specification of the addition\n",
    "problem:\n",
    "- **n_chars**: The size of the alphabet for a single time step (e.g. 12 for 0-9, '+' and ' ').\n",
    "- **n_in_seq_length**: The number of time steps of encoded input sequences (e.g. 8 for '10+10+10').\n",
    "- **n_out_seq_length**: The number of time steps of an encoded output sequence (e.g. 2 for '30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configure problem\n",
    "\n",
    "# number of math terms\n",
    "n_numbers = 3\n",
    "# largest value for any single input digit\n",
    "largest = 10\n",
    "# scope of possible symbols for each input or output time step\n",
    "alphabet = [str(x) for x in range(10)] + ['+', ' ']\n",
    "\n",
    "# size of alphabet: (12 for 0-9, + and ' ')\n",
    "n_chars = len(alphabet)\n",
    "# length of encoded input sequence (8 for '10+10+10)\n",
    "n_in_seq_length = n_numbers * ceil(log10(largest+1)) + n_numbers - 1\n",
    "# length of encoded output sequence (2 for '30')\n",
    "n_out_seq_length = ceil(log10(n_numbers * (largest+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Model compilation\n",
    "\n",
    "We are now ready to define the Encoder-Decoder LSTM. We will\n",
    "\n",
    "- Use a single LSTM layer for the encoder and another single layer for the decoder. \n",
    "- Define the encoder with 75 memory cells.\n",
    "- Define the decoder with 50 memory cells. \n",
    "- There is nothing special about the number of memory cells. It was found with a little trial and error. The asymmetry in layer sizes in the encoder and decoder seems like a natural organization given that input sequences are relatively longer than output sequences.\n",
    "- Use the [categorical log loss](http://www.exegetic.biz/blog/2015/12/making-sense-logarithmic-loss/) for the 11 possible classes ('+' is not in the output sequece) that may be predicted in the output layer.\n",
    "- Use the eficient Adam implementation of gradient descent and calculate accuracy during training and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "\n",
    "# define LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(75, input_shape=(n_in_seq_length, n_chars)))\n",
    "model.add(RepeatVector(n_out_seq_length))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_chars, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, yes, I hear you, are we using over 52 K parameters to solve a simple addition problem!? But oh well, this is just to illustrate the technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Fit the model\n",
    "The model is fit on a single epoch of 75,000 randomly generated instances of input-output pairs.\n",
    "The number of sequences is a proxy for the number of training epochs. The total of 75,000\n",
    "and a batch size of 32 were found with a little trial and error and are by no means an optimal\n",
    "configuration. You are encouraged to play with this parameters to get a feeling about the fitting of the model.\n",
    "\n",
    "Fitting the model provides a progress bar that shows the loss and accuracy of the model at\n",
    "the end of each batch. The model does not take long to fit on the CPU. If the progress bar\n",
    "interferes with your development environment, you can turn it off by setting verbose=0 in the\n",
    "call to the fit() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit LSTM\n",
    "X, y = generate_data(75000, n_numbers, largest, alphabet)\n",
    "model.fit(X, y, epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Evaluate the model\n",
    "We can evaluate the model by generating predictions on 10000 different randomly generated\n",
    "input-output pairs. The result will give an estimate of the model skill on randomly generated\n",
    "examples in general.\n",
    "\n",
    "Running the example prints both the log loss and accuracy of the model. Your speci\f",
    "c\n",
    "values may differ because of the stochastic nature of neural networks, but the model accuracy\n",
    "should be in the high 90s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate LSTM\n",
    "X, y = generate_data(10000, n_numbers, largest, alphabet)\n",
    "loss, acc = model.evaluate(X, y, verbose=0)\n",
    "print('Loss: %f, Accuracy: %f' % (loss, acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Make Predictions with the Model\n",
    "\n",
    "We can make predictions using the fit model. We will demonstrate making one prediction at\n",
    "a time and provide a summary of the decoded input, expected output, and predicted output.\n",
    "Printing the decoded output gives us a more concrete connection to the problem and model\n",
    "performance. Here, we generate 10 new random input-output sequence pairs, make a prediction\n",
    "using the fit model for each, decode all the sequences involved, and print them to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fancy printing in colors\n",
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'\n",
    "    \n",
    "# predict\n",
    "for _ in range(10):\n",
    "    # generate an input-output pair\n",
    "    X, y = generate_data(1, n_numbers, largest, alphabet)\n",
    "    # make prediction\n",
    "    yhat = model.predict(X, verbose=0)\n",
    "    # decode input, expected and predicted\n",
    "    out_seq = invert(y[0], alphabet)\n",
    "    predicted = invert(yhat[0], alphabet)\n",
    "    if out_seq == predicted:   \n",
    "        print(colors.ok + '☑' + colors.close, end=' ')\n",
    "    else:\n",
    "        print(colors.fail + '☒' + colors.close, end=' ')\n",
    "    print('%s = %s (when expecting %s)' % (in_seq, predicted, out_seq))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code several times in the cell above and look at the predictions. Does the model get everything right all the time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model\n",
    "\n",
    "You can save the model architecture (layers and how they connect) and weights (arrays\n",
    "of numbers) to separate files. This is the recommended approach as it allows you to update the\n",
    "model weights at a later stage while ensuring the model architecture is left unchanged. Maybe you want to update your model weights without changing the architecture?\n",
    "\n",
    "Keras provides two formats for preserving the model architecture: JSON and YAML formats.\n",
    "The difference being in having a human readable format or not. \n",
    "The following code sample saves the architecture in a JSON file. \n",
    "The weights are saved in a [HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert model architecture to JSON format\n",
    "architecture = model.to_json()\n",
    "# save architecture to JSON file\n",
    "with open('architecture.json', 'wt') as json_file:\n",
    "    json_file.write(architecture)\n",
    "# save weights to hdf5 file\n",
    "model.save_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# load architecture from JSON File\n",
    "json_file = open('architecture.json', 'rt')\n",
    "architecture = json_file.read()\n",
    "json_file.close()\n",
    "# create model from architecture\n",
    "loaded_model = model_from_json(architecture)\n",
    "# load weights from hdf5 file\n",
    "loaded_model.load_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'\n",
    "\n",
    "# make predictions again\n",
    "for _ in range(10):\n",
    "    # generate an input-output pair\n",
    "    X, y = generate_data(1, n_numbers, largest, alphabet)\n",
    "    # make prediction\n",
    "    yhat = loaded_model.predict(X, verbose=0)\n",
    "    # decode input, expected and predicted\n",
    "    in_seq = invert(X[0], alphabet)\n",
    "    out_seq = invert(y[0], alphabet)\n",
    "    predicted = invert(yhat[0], alphabet)\n",
    "    if out_seq == predicted:   \n",
    "        print(colors.ok + '☑' + colors.close, end=' ')\n",
    "    else:\n",
    "        print(colors.fail + '☒' + colors.close, end=' ')\n",
    "    print('%s = %s (when expecting %s)' % (in_seq, predicted, out_seq))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to do next?\n",
    "\n",
    "Congratulations! You have come so far. Now take a look at the homework and play a bit more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
