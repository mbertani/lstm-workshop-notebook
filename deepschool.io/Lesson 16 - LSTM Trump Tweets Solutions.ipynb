{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "The following notebook is borrowed from [deepschool.io](https://github.com/sachinruk/deepschool.io/blob/master/Lesson%2016%20-%20LSTM%20Trump%20Tweets%20-%20Solutions.ipynb). I have refactored the code a bit, added some print statements to clarify what things look like, and modified the bit that saves the model, to separate the architecture from the weights.\n",
    "Then I pretrained some models, that you can load to see the effect of different sizes of the LSTM used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM (Long Short Term Memory)\n",
    "\n",
    "There is a branch of Deep Learning that is dedicated to processing time series. These deep Nets are **Recursive Neural Nets (RNNs)**. LSTMs are one of the few types of RNNs that are available. Gated Recurent Units (GRUs) are the other type of popular RNNs.\n",
    "\n",
    "This is an illustration from http://colah.github.io/posts/2015-08-Understanding-LSTMs/ (A highly recommended read)\n",
    "\n",
    "![RNNs](./images/RNN-unrolled.png)\n",
    "\n",
    "Pros:\n",
    "- Really powerful pattern recognition system for time series\n",
    "\n",
    "Cons:\n",
    "- Cannot deal with missing time steps.\n",
    "- Time steps must be discretised and not continuous.\n",
    "\n",
    "![trump](./images/trump.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization, LSTM, Embedding, TimeDistributed\n",
    "from keras.models import load_model, model_from_json\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/trump.csv') # might need to change location if on Floydhub\n",
    "df = df[df.is_retweet=='false']\n",
    "df.text = df.text.str.lower()\n",
    "df.text = df.text.str.replace(r'http[\\w:/\\.]+','') # remove urls\n",
    "df.text = df.text.str.replace(r'[^!\\'\"#$%&\\()*+,-./:;<=>?@_â€™`{|}~\\w\\s]',' ') #remove everything but characters and punctuation\n",
    "df.text = df.text.str.replace(r'\\s\\s+',' ') #replace multple white space with a single one\n",
    "df = df[[len(t)<180 for t in df.text.values]]\n",
    "df = df[[len(t)>50 for t in df.text.values]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweets = [text for text in df.text.values[::-1]]\n",
    "trump_tweets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary to convert letters to numbers and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = ''.join(trump_tweets)\n",
    "char2int = dict(zip(set(all_tweets), range(len(set(all_tweets)))))\n",
    "char2int['<END>'] = len(char2int)\n",
    "char2int['<GO>'] = len(char2int)\n",
    "char2int['<PAD>'] = len(char2int)\n",
    "int2char = dict(zip(char2int.values(), char2int.keys()))\n",
    "\n",
    "# Print the dictionaries extracted from the data\n",
    "print(char2int)\n",
    "print(int2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the text representation of the tweets to be used with neural networks\n",
    "text_num = [[char2int['<GO>']]+[char2int[c] for c in tweet]+ [char2int['<END>']] for tweet in trump_tweets]\n",
    "\n",
    "# Print an example to see what is going on\n",
    "print('<GO>' + trump_tweets[0] +'<END>')\n",
    "print(text_num[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([len(t) for t in trump_tweets],50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_vocab = len(char2int)\n",
    "sentence_len = 40\n",
    "\n",
    "num_examples = 0\n",
    "for tweet in text_num:\n",
    "    num_examples += len(tweet)-sentence_len\n",
    "\n",
    "x = np.zeros((num_examples, sentence_len))\n",
    "y = np.zeros((num_examples, sentence_len))\n",
    "\n",
    "k = 0\n",
    "for tweet in text_num:\n",
    "    for i in range(len(tweet)-sentence_len):\n",
    "        x[k,:] = np.array(tweet[i:i+sentence_len])\n",
    "        y[k,:] = np.array(tweet[i+1:i+sentence_len+1])\n",
    "        k += 1\n",
    "        \n",
    "y = y.reshape(y.shape+(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Many to Many LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len_vocab, 512)) # , batch_size=batch_size\n",
    "model.add(LSTM(512, return_sequences=True)) # , stateful=True\n",
    "model.add(TimeDistributed(Dense(len_vocab, activation='softmax')))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay special attention to how the probabilites are taken. p is of shape `(1, sequence_len, len(char2int))` where len(char2int) is the number of available characters. The 1 is there because we are only predicting one feature, `y`. We are only concerned about the last prediction probability of the sequence. This is due to the fact that all other letters have already been appended. Hence we predict a letter from the distribution `p[0][-1]`.\n",
    "\n",
    "Why did we keep appending to the sequence and predicting? Why not use simply the last letter. If we were to do this, we would lose information that comes from the previous letter via the hidden state and cell memory. Keep in mind that each LSTM unit has 3 inputs, the x, the hidden state, and the cell memory. \n",
    "\n",
    "Also important to notice that the Cell Memory is not used in connecting to the Dense layer, only the hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(5,20,p=[0.9, 0.1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to predict tweets\n",
    "# encoded_sentence: integer array of encoded tweet\n",
    "# model: the model used to predict the probabilities of each character\n",
    "# encode_dict: dictionary to convert from char to integers\n",
    "# decode_dict: dictionary to convert from integers to chars\n",
    "# returns a string with the predicted tweet, predicting one char at at time\n",
    "def predict_tweet(encoded_sentence, model,encode_dict,decode_dict):\n",
    "    sentence = [decode_dict[l] for l in encoded_sentence]\n",
    "    for i in range(150):\n",
    "        if sentence[-1]=='<END>':\n",
    "            break\n",
    "        p = model.predict(np.array(encoded_sentence)[None,:])\n",
    "        encoded_sentence.append(np.random.choice(len(encode_dict),1,p=p[0][-1])[0])\n",
    "        sentence.append(decode_dict[encoded_sentence[-1]])\n",
    "    return ''.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model over 10 epochs, and print a prediction for each epoch\n",
    "# If you just want a quick result, just train it one epoch by setting n_epochs to 1\n",
    "n_epochs = 10\n",
    "for i in range(n_epochs+1):\n",
    "    print('<'*100)\n",
    "    print( 'Start training epoch %s' % (i))\n",
    "    tweet = [char2int['<GO>']] #choose a random letter\n",
    "    print(predict_tweet(tweet,model,char2int,int2char)) # print a random tweet predicted by model so far\n",
    "    print('>'*100)\n",
    "    if i!=n_epochs:\n",
    "        model.fit(x,y, batch_size=1024, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "\n",
    "There are actually three things that need to be saved when saving RNN models in keras.\n",
    "1. The model architecture. \n",
    "2. The model weights. It's a good idea to separate the architecture from the weights, if you decide to retrain your model afterwards.\n",
    "3. The associated dictionary that refers to the character embeddings. This is due to the fact that in Python the dictionaries are not created the same way at each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Convert model architecture to JSON format\n",
    "architecture = model.to_json()\n",
    "\n",
    "# save architecture to JSON file\n",
    "with open('./architecture.trump.json', 'wt') as json_file:\n",
    "    json_file.write(architecture)\n",
    "\n",
    "# 2. Save weights to hdf5 file\n",
    "model.save_weights('./weights.trump.h5')\n",
    "\n",
    "# 3. Save the dictionary for the character embeddings\n",
    "with open('./tweets.pickle', 'wb') as f:\n",
    "    pickle.dump((char2int, int2char), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the model run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load text Dict\n",
    "with open('./tweets.pickle', 'rb') as f:\n",
    "    char2int, int2char = pickle.load(f)\n",
    "\n",
    "# load architecture from JSON File\n",
    "json_file = open('./architecture.trump.json', 'rt')\n",
    "architecture = json_file.read()\n",
    "json_file.close()\n",
    "# create model from architecture\n",
    "model2 = model_from_json(architecture)\n",
    "# load weights from hdf5 file\n",
    "model2.load_weights('./weights.trump.h5')\n",
    "#model2 = model2.load_weights('trump_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict 5 tweets with your trained model\n",
    "for j in range(10):\n",
    "    tweet = [char2int['<GO>']] #choose a random letter\n",
    "    print(predict_tweet(tweet,model2,char2int,int2char))\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete some tweets\n",
    "\n",
    "tweet = [char2int[letter] for letter in \"white supremacists are \"]\n",
    "print(predict_tweet(tweet,model2,char2int,int2char) + \"\\n\")\n",
    "\n",
    "tweet = [char2int[letter] for letter in \"obama is \"]\n",
    "print(predict_tweet(tweet,model2,char2int,int2char)+ \"\\n\")\n",
    "\n",
    "tweet = [char2int[letter] for letter in \"i resign\"]\n",
    "print(predict_tweet(tweet,model2,char2int,int2char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the pretrained models\n",
    "In the following, we load the models I have pretrained for you. I have used the same architecture, but just played a bit with the size of the Embedding network for sizes of:\n",
    "\n",
    "- 64 , epochs: 40, loss: 1.5484\n",
    "- 128, epochs: 30, loss: 1.39   \n",
    "- 256, epochs: 20, loss: 1.1716 \n",
    "- 512, epochs: 10, loss: 0.9269\n",
    "\n",
    "We load each model, and make some predictions. Look at how the predictions evolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Colors for printing\n",
    "class colors:\n",
    "    RED = '\\033[91m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'    \n",
    "    close = '\\033[0m'\n",
    "\n",
    "# Function to load the pretrained models\n",
    "def load_model(dictionary_file, architecture_file, weight_file):\n",
    "    # Load text Dict\n",
    "    with open(dictionary_file, 'rb') as f:\n",
    "        encode_dict, decode_dict = pickle.load(f)\n",
    "\n",
    "    # load architecture from JSON File\n",
    "    json_file = open(architecture_file, 'rt')\n",
    "    architecture = json_file.read()\n",
    "    json_file.close()\n",
    "\n",
    "    # create model from architecture\n",
    "    model = model_from_json(architecture)\n",
    "    # load weights from hdf5 file\n",
    "    model.load_weights(weight_file)\n",
    "    return encode_dict, decode_dict, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained models\n",
    "\n",
    "char2int_x64, int2char_x64, model_x64 = load_model('./tweets.x64.e40.pickle', './architecture.x64.e40.trump.json', './weights.x64.e40.trump.h5')\n",
    "char2int_x128, int2char_x128, model_x128 = load_model('./tweets.x128.e30.pickle', './architecture.x128.e30.trump.json', './weights.x128.e30.trump.h5')\n",
    "char2int_x256, int2char_x256, model_x256 = load_model('./tweets.x256.e20.pickle', './architecture.x256.e20.trump.json', './weights.x256.e20.trump.h5')\n",
    "char2int_x512, int2char_x512, model_x512 = load_model('./tweets.x512.e10.pickle', './architecture.x512.e10.trump.json', './weights.x512.e10.trump.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict 5 tweets of each model\n",
    "\n",
    "print(colors.BLUE + '='*50 + ' x64e40 ' + '='*50 + colors.close)\n",
    "for j in range(5):\n",
    "    tweet = [char2int_x64['<GO>']] #choose a random letter\n",
    "    print(colors.BLUE + predict_tweet(tweet,model_x64,char2int_x64,int2char_x64) + colors.close)    \n",
    "\n",
    "print(colors.GREEN + '='*50 + ' x128e30 ' + '='*50 + colors.close)\n",
    "for j in range(5):\n",
    "    tweet = [char2int_x128['<GO>']] #choose a random letter\n",
    "    print(colors.GREEN + predict_tweet(tweet,model_x128,char2int_x128,int2char_x128) + colors.close)    \n",
    "    \n",
    "print(colors.RED + '='*50 + ' x256e20 ' + '='*50 + colors.close)\n",
    "for j in range(5):\n",
    "    tweet = [char2int_x256['<GO>']] #choose a random letter\n",
    "    print(colors.RED + predict_tweet(tweet,model_x256,char2int_x256,int2char_x256) + colors.close)    \n",
    "\n",
    "print('='*50 + ' x512e10 ' + '='*50)\n",
    "for j in range(5):\n",
    "    tweet = [char2int_x512['<GO>']] #choose a random letter\n",
    "    print(predict_tweet(tweet,model_x512,char2int_x512,int2char_x512))    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = [char2int_x64[letter] for letter in \"white supremacists are \"]\n",
    "print(colors.BLUE + 'x64e40: ' +predict_tweet(tweet,model_x64,char2int_x64,int2char_x64) + colors.close + \"\\n\")\n",
    "\n",
    "tweet = [char2int_x128[letter] for letter in \"white supremacists are \"]\n",
    "print(colors.GREEN + 'x128e30: ' + predict_tweet(tweet,model_x128,char2int_x128,int2char_x128) + colors.close +\"\\n\")\n",
    "\n",
    "tweet = [char2int_x256[letter] for letter in \"white supremacists are \"]\n",
    "print(colors.RED + 'x256e20: ' + predict_tweet(tweet,model_x256,char2int_x256,int2char_x256) + colors.close +\"\\n\")\n",
    "\n",
    "tweet = [char2int_x512[letter] for letter in \"white supremacists are \"]\n",
    "print('x512e10: ' + predict_tweet(tweet,model_x512,char2int_x512,int2char_x512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = [char2int_x64[letter] for letter in \"obama is \"]\n",
    "print(colors.BLUE + 'x64e40: ' +predict_tweet(tweet,model_x64,char2int_x64,int2char_x64) + colors.close + \"\\n\")\n",
    "\n",
    "tweet = [char2int_x128[letter] for letter in \"obama is \"]\n",
    "print(colors.GREEN + 'x128e30: ' + predict_tweet(tweet,model_x128,char2int_x128,int2char_x128) + colors.close + \"\\n\")\n",
    "\n",
    "tweet = [char2int_x256[letter] for letter in \"obama is \"]\n",
    "print(colors.RED + 'x256e20: ' + predict_tweet(tweet,model_x256,char2int_x256,int2char_x256) + colors.close + \"\\n\")\n",
    "\n",
    "tweet = [char2int_x512[letter] for letter in \"obama is \"]\n",
    "print('x512e10: ' + predict_tweet(tweet,model_x512,char2int_x512,int2char_x512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = [char2int_x64[letter] for letter in \"i resign \"]\n",
    "print(colors.BLUE + 'x64e40: ' +predict_tweet(tweet,model_x64,char2int_x64,int2char_x64) + colors.close + \"\\n\")\n",
    "\n",
    "tweet = [char2int_x128[letter] for letter in \"i resign \"]\n",
    "print(colors.GREEN + 'x128e30: ' + predict_tweet(tweet,model_x128,char2int_x128,int2char_x128) + colors.close + \"\\n\")\n",
    "\n",
    "tweet = [char2int_x256[letter] for letter in \"i resign \"]\n",
    "print(colors.RED + 'x256e20: ' + predict_tweet(tweet,model_x256,char2int_x256,int2char_x256) + colors.close+ \"\\n\")\n",
    "\n",
    "tweet = [char2int_x512[letter] for letter in \"i resign \"]\n",
    "print('x512e10: ' + predict_tweet(tweet,model_x512,char2int_x512,int2char_x512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = [char2int_x512[letter] for letter in \"covfefe means \"]\n",
    "print('x512e10: ' + predict_tweet(tweet,model_x512,char2int_x512,int2char_x512))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
